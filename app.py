import streamlit as st
import tempfile
import os
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ“ˆ ì£¼ì‹ í¬íŠ¸í´ë¦¬ì˜¤ AI ë¶„ì„",
    page_icon="ğŸ“Š",
    layout="wide"
)

# ì»¤ìŠ¤í…€ CSS
st.markdown("""
<style>
    .stChat message {
        padding: 1rem;
        border-radius: 0.5rem;
    }
    .main-header {
        text-align: center;
        padding: 1rem;
        background: linear-gradient(90deg, #1e3a5f, #2d5a87);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .sidebar-info {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 10px;
        margin-top: 1rem;
    }
</style>
""", unsafe_allow_html=True)

# í—¤ë”
st.markdown("""
<div class="main-header">
    <h1>ğŸ“ˆ ì£¼ì‹ í¬íŠ¸í´ë¦¬ì˜¤ AI ë¶„ì„ ì±—ë´‡</h1>
    <p>PDF ë¬¸ì„œ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ | Powered by Gemini 2.5 Flash</p>
</div>
""", unsafe_allow_html=True)

# API í‚¤ ì„¤ì •
try:
    GOOGLE_API_KEY = st.secrets["GEMINI_API_KEY"]
except Exception:
    st.error("âš ï¸ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsì— 'GEMINI_API_KEY'ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
    st.stop()

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì£¼ì‹ í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„ ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

[ì—­í• ]
- ì—…ë¡œë“œëœ PDF ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì£¼ì‹ ë° íˆ¬ì ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.
- ì‹œí™© ë¶„ì„, í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±, ë¦¬ìŠ¤í¬ ê´€ë¦¬ì— ëŒ€í•œ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤.

[ë‹µë³€ ì›ì¹™]
1. ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ(Context)ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
2. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ì— ëŒ€í•´ì„œëŠ” "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ëª…í™•íˆ ë§í•˜ì„¸ìš”.
3. íˆ¬ì ì¡°ì–¸ ì‹œ í•­ìƒ "íˆ¬ìì˜ ì±…ì„ì€ ë³¸ì¸ì—ê²Œ ìˆìŠµë‹ˆë‹¤"ë¼ëŠ” ë©´ì±… ì¡°í•­ì„ í¬í•¨í•˜ì„¸ìš”.
4. ë‹µë³€ì€ êµ¬ì²´ì ì´ê³  ì²´ê³„ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

[Context]
{context}

[ëŒ€í™” ê¸°ë¡]
{chat_history}

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

[ë‹µë³€]
"""

@st.cache_resource
def get_llm():
    """Gemini 2.5 Flash ëª¨ë¸ ì´ˆê¸°í™”"""
    return ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        google_api_key=GOOGLE_API_KEY,
        temperature=0.3,
        convert_system_message_to_human=True
    )

@st.cache_resource
def get_embeddings():
    """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
    return GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=GOOGLE_API_KEY
    )

def process_pdf(pdf_file):
    """PDF íŒŒì¼ ì²˜ë¦¬ ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(pdf_file.read())
        tmp_path = tmp_file.name

    try:
        # PDF ë¡œë“œ
        loader = PyPDFLoader(tmp_path)
        documents = loader.load()

        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
        )
        splits = text_splitter.split_documents(documents)

        # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        embeddings = get_embeddings()
        vectorstore = FAISS.from_documents(splits, embeddings)

        return vectorstore, len(documents), len(splits)

    finally:
        os.unlink(tmp_path)

def create_chain(vectorstore):
    """ëŒ€í™”í˜• ê²€ìƒ‰ ì²´ì¸ ìƒì„±"""
    llm = get_llm()

    prompt = PromptTemplate(
        input_variables=["context", "chat_history", "question"],
        template=SYSTEM_PROMPT
    )

    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )

    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(search_kwargs={"k": 4}),
        memory=memory,
        combine_docs_chain_kwargs={"prompt": prompt},
        return_source_documents=True,
        verbose=False
    )

    return chain

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ“‚ ë¬¸ì„œ ì—…ë¡œë“œ")

    uploaded_file = st.file_uploader(
        "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=["pdf"],
        help="ì£¼ì‹ ë¦¬í¬íŠ¸, í¬íŠ¸í´ë¦¬ì˜¤ ë¬¸ì„œ ë“±"
    )

    # ê¸°ë³¸ test.pdf ì‚¬ìš© ì˜µì…˜
    use_default = st.checkbox("ê¸°ë³¸ test.pdf ì‚¬ìš©", value=False)

    if use_default and os.path.exists("test.pdf"):
        with open("test.pdf", "rb") as f:
            uploaded_file = f
            st.success("âœ… test.pdf ë¡œë“œë¨")

    st.divider()

    # ë¬¸ì„œ ì²˜ë¦¬
    if uploaded_file is not None:
        if st.button("ğŸ”„ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘", type="primary", use_container_width=True):
            with st.spinner("ğŸ“„ PDF ë¶„ì„ ì¤‘..."):
                try:
                    vectorstore, num_pages, num_chunks = process_pdf(uploaded_file)
                    st.session_state.vectorstore = vectorstore
                    st.session_state.chain = create_chain(vectorstore)
                    st.session_state.doc_processed = True

                    st.success(f"""
                    âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ!
                    - ì´ í˜ì´ì§€: {num_pages}
                    - ë¶„í•  ì²­í¬: {num_chunks}
                    """)
                except Exception as e:
                    st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    st.divider()

    # ì •ë³´ íŒ¨ë„
    st.markdown("""
    <div class="sidebar-info">
    <h4>ğŸ’¡ ì‚¬ìš© ë°©ë²•</h4>
    <ol>
        <li>PDF íŒŒì¼ ì—…ë¡œë“œ</li>
        <li>'ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘' í´ë¦­</li>
        <li>ì±„íŒ…ìœ¼ë¡œ ì§ˆë¬¸í•˜ê¸°</li>
    </ol>
    </div>
    """, unsafe_allow_html=True)

    st.markdown("""
    <div class="sidebar-info">
    <h4>ğŸ“Š ì§ˆë¬¸ ì˜ˆì‹œ</h4>
    <ul>
        <li>í˜„ì¬ ì‹œì¥ ìƒí™©ì€ ì–´ë–¤ê°€ìš”?</li>
        <li>ì¶”ì²œ í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±ì€?</li>
        <li>ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë°©ë²•ì€?</li>
        <li>ì„¹í„°ë³„ ì „ë§ì„ ì•Œë ¤ì£¼ì„¸ìš”</li>
    </ul>
    </div>
    """, unsafe_allow_html=True)

    if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# ë©”ì¸ ì±„íŒ… ì˜ì—­
col1, col2 = st.columns([3, 1])

with col1:
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []

    if "doc_processed" not in st.session_state:
        st.session_state.doc_processed = False

    # ì±„íŒ… ì»¨í…Œì´ë„ˆ
    chat_container = st.container()

    with chat_container:
        # ì´ì „ ë©”ì‹œì§€ í‘œì‹œ
        for message in st.session_state.messages:
            with st.chat_message(message["role"], avatar="ğŸ§‘â€ğŸ’¼" if message["role"] == "user" else "ğŸ¤–"):
                st.markdown(message["content"])

        # ë¬¸ì„œ ë¯¸ì²˜ë¦¬ ì‹œ ì•ˆë‚´
        if not st.session_state.doc_processed:
            st.info("ğŸ‘ˆ ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")

    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("ì£¼ì‹ í¬íŠ¸í´ë¦¬ì˜¤ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”...", disabled=not st.session_state.doc_processed):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})

        with st.chat_message("user", avatar="ğŸ§‘â€ğŸ’¼"):
            st.markdown(prompt)

        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            with st.spinner("ë¶„ì„ ì¤‘..."):
                try:
                    chain = st.session_state.chain
                    response = chain({"question": prompt})
                    answer = response["answer"]

                    # ì†ŒìŠ¤ ë¬¸ì„œ ì •ë³´
                    sources = response.get("source_documents", [])

                    st.markdown(answer)

                    # ì°¸ì¡° ë¬¸ì„œ í‘œì‹œ
                    if sources:
                        with st.expander("ğŸ“š ì°¸ì¡° ë¬¸ì„œ"):
                            for i, doc in enumerate(sources[:3]):
                                st.markdown(f"**[{i+1}]** {doc.page_content[:200]}...")

                    st.session_state.messages.append({"role": "assistant", "content": answer})

                except Exception as e:
                    error_msg = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                    st.error(error_msg)
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})

with col2:
    st.markdown("### ğŸ“ˆ ë¹ ë¥¸ ë¶„ì„")

    if st.session_state.doc_processed:
        quick_questions = [
            "ğŸ“Š ì „ì²´ ìš”ì•½",
            "ğŸ’° íˆ¬ì ì¶”ì²œ",
            "âš ï¸ ë¦¬ìŠ¤í¬ ë¶„ì„",
            "ğŸ”® ì‹œì¥ ì „ë§"
        ]

        for q in quick_questions:
            if st.button(q, use_container_width=True):
                # ë²„íŠ¼ í´ë¦­ ì‹œ í•´ë‹¹ ì§ˆë¬¸ìœ¼ë¡œ ì±„íŒ…
                question_map = {
                    "ğŸ“Š ì „ì²´ ìš”ì•½": "ë¬¸ì„œì˜ ì „ì²´ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.",
                    "ğŸ’° íˆ¬ì ì¶”ì²œ": "í˜„ì¬ ì¶”ì²œí•˜ëŠ” íˆ¬ì ì „ëµê³¼ í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    "âš ï¸ ë¦¬ìŠ¤í¬ ë¶„ì„": "í˜„ì¬ ì‹œì¥ì˜ ì£¼ìš” ë¦¬ìŠ¤í¬ ìš”ì¸ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.",
                    "ğŸ”® ì‹œì¥ ì „ë§": "í–¥í›„ ì‹œì¥ ì „ë§ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”."
                }
                st.session_state.quick_question = question_map[q]
                st.rerun()
    else:
        st.caption("ë¬¸ì„œ ì²˜ë¦¬ í›„ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")

# ë¹ ë¥¸ ì§ˆë¬¸ ì²˜ë¦¬
if "quick_question" in st.session_state:
    prompt = st.session_state.quick_question
    del st.session_state.quick_question

    st.session_state.messages.append({"role": "user", "content": prompt})

    try:
        chain = st.session_state.chain
        response = chain({"question": prompt})
        st.session_state.messages.append({"role": "assistant", "content": response["answer"]})
    except Exception as e:
        st.session_state.messages.append({"role": "assistant", "content": f"ì˜¤ë¥˜: {str(e)}"})

    st.rerun()

# í‘¸í„°
st.divider()
st.caption("âš ï¸ ë³¸ ì„œë¹„ìŠ¤ëŠ” ì°¸ê³ ìš©ì´ë©°, ì‹¤ì œ íˆ¬ì ê²°ì •ì€ ì „ë¬¸ê°€ì™€ ìƒë‹´ í›„ ë³¸ì¸ì˜ ì±…ì„í•˜ì— ì´ë£¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤.")
